{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section, we figured out how to interact with the Twitter API. Now we need to pull more data. 1000 tweets, or 10 requests, should be enough to play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple pieces that go into acquiring clean data :\n",
    "\n",
    "1. We need to figure out a way to make multiple requests, but without pulling duplicate tweets.\n",
    "\n",
    "2. We need to parse the request output into a form and then put it into an form that can be analyzed - i.e. a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tackling 1:\n",
    "\n",
    "From https://dev.twitter.com/rest/public/timelines:\n",
    "\n",
    "\"To use max_id correctly, an application’s first request to a timeline endpoint should only specify a count. When processing this and subsequent responses, keep track of the lowest ID received. This ID should be passed as the value of the max_id parameter for the next request, which will only return Tweets with IDs lower than or equal to the value of the max_id parameter. Note that the max_id parameter is inclusive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import pandas as pd\n",
    "import python.twitter_authentication as twit_auth\n",
    "twitter_api = twit_auth.authenticate_twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.29 µs\n",
      "Getting tweets 100 to 200\n",
      "Getting tweets 200 to 300\n",
      "Getting tweets 300 to 400\n",
      "Getting tweets 400 to 500\n"
     ]
    }
   ],
   "source": [
    "% time\n",
    "SEARCHTERM = \"Super Bowl\"\n",
    "n = 500\n",
    "min_index = 99999999999999999999\n",
    "\n",
    "data_types = ['id', 'text', 'retweet_count']\n",
    "\n",
    "tweets_dict = {}\n",
    "tweets_dict['id'] = []\n",
    "tweets_dict['text'] = []\n",
    "tweets_dict['retweet_count'] = []\n",
    "\n",
    "# initial search without max_id parameter\n",
    "search = twitter_api.search.tweets(q=SEARCHTERM, count=100)\n",
    "results = list(search.values())\n",
    "\n",
    "for data in data_types:\n",
    "    for i in range(100):\n",
    "        tweets_dict[data].append(results[0][i][data])\n",
    "        if data == 'id' and results[0][i][data] < min_index:\n",
    "            min_index = results[0][i][data]\n",
    "\n",
    "# now repeat the request to get rest of results,\n",
    "# setting max_id to the lowest id - 1 (to avoid duplicate tweets)\n",
    "for i in range(n // 100 - 1):\n",
    "    print('Getting tweets', (i+1)*100, 'to', (i+2)*100)\n",
    "    search = twitter_api.search.tweets(q=SEARCHTERM, \n",
    "                                       count=100, \n",
    "                                       max_id=str(min_index))\n",
    "    results = list(search.values())\n",
    "\n",
    "    for data in data_types:\n",
    "        for i in range(100):\n",
    "            tweets_dict[data].append(results[0][i][data])\n",
    "            if data == 'id' and results[0][i][data] < min_index:\n",
    "                min_index = results[0][i][data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(tweets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we need to validate that it was scraped correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "2      4\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['id'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. We can see that we have 1000 unique tweets."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
